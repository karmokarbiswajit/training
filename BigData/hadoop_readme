Local Location
/Volumes/BiswajitData/Dropbox/Training

set -o vi
hostname -f

ssh cloudera@10.0.0.5 -p22

copy file from local to vm (get ip address from eth1)
scp -p22 crime.json cloudera@10.0.0.5:/home/cloudera/files

sudo service hadoop-yarn-nodemanager status
sudo service hadoop-yarn-resourcemanager status

after upgrading to cloudera express run the below
sudo su - hdfs
hadoop fs -chmod -R 777 /user/spark

hadoop fs gives list of commands available
hadoop fs -tail /user/output/part

hadoop fs -rm -r /user/cloudera/sqoop_import

sudo -u hdfs hadoop fs -chmod 777 /user/cloudera/input_files
hadoop fs -rm -r folder_name

Get file size
dfs -du -s -h

view /etc/hadoop/conf/core-site.xml

hive> dfs -du -s -h user/hive/warehouse/tablename ---gives size
hive> dfs ------- run hadoop cmds in hive
grunt> fs ------  run hadoop cmds in pig

####CREATE FOLDER IN HDFS AND COPY FILES####
sudo -u hdfs hadoop fs -mkdir /user/cloudera/input_files
hadoop fs -ls /user/cloudera/input_files
sudo -u hdfs hadoop fs -put /home/cloudera/Downloads/Input/DelayedFlights.csv /user/cloudera/input_files
hadoop fs -cat /user/cloudera/input_files/DelayedFlights.csv | wc -l

#######MYSQL#######
mysql -u root -p
cloudera

loads tables with its structures as mentioned in the SQL.
mysql -t < employees.sql
mysql -u horton -p -t < employees.sql
