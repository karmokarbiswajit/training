# create a file with pyspark code and run the below command to execute the code
# spark-submit --master yarn filename.py

----------------------------
1. GET DATA FROM HIVE TABLE
----------------------------

from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
from pyspark.sql import HiveContext

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)

sqlContext = HiveContext(sc)

dept = sqlContext.sql("select * from retail_db.departments")

for rec in dept.collect():
  print(rec)

------------------------------
2. GET DATA FROM MYSQL TABLE
------------------------------
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
sqlContext = SQLContext(sc)

mysqlcred = "jdbc:mysql://localhost:3306/retail_db?user=root&password=cloudera"

df = sqlContext.load(source="jdbc", url=mysqlcred, dbtable="retail_db.departments")

for rec in df.collect():
  print(rec)

--------------------------------------------------
3. READ FROM A FILE and save data on another file
   and SEQUENCE file
--------------------------------------------------
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)

dataRDD = sc.textFile("/user/root/input/state_table.csv")

dataRDD.saveAsTextFile("/user/root/output/spark_op3.1")
dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/user/root/output/spark_op3.2")

--------------------------------------------------
4. LOAD JSON FILE
--------------------------------------------------
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
sqlContext = SQLContext(sc)

airfieldsjson = sqlContext.jsonFile("/user/root/input/airfields.json")
airfieldsjson.registerTempTable("t_airfields_json")
airfieldsdata = sqlContext.sql("select * from t_airfields_json")
for rec in airfieldsdata.collect():
  print(rec)

------------------------
NOTES
------------------------
READING SEQUENCE FILE
data = sc.sequenceFile("/user/root/output/spark_op3.2")
for i in data.collect():
  print(i)
  
.take(5) --get 5 records
  


